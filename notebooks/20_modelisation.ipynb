{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from functools import partial\n",
    "import datapane as dp\n",
    "\n",
    "import sklearn.pipeline as skpip\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from category_encoders import WOEEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import my libs\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.features.balance import BalanceMixin\n",
    "from src.features.correlation import HighCorrelation_filter\n",
    "from src.features.variance import NearZeroVar_filter, LowVar_Filter\n",
    "from src.features.passthrough import Passthrough\n",
    "\n",
    "from src.models.binary_classification.algos.logistic_autotuning import LogisticAutoTuning\n",
    "from src.models.binary_classification.algos.xgboost_autotuning import XGBoostAutoTuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modelisationTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/modelling_tuple', 'rb') as f:\n",
    "    M = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function I use in preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _intersect(df: pd.DataFrame, target: list):\n",
    "    \"\"\"\n",
    "    Intersection between a list and a set of columns in a dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe with which we want to make the intersection.\n",
    "        target (list): The array with which we want to make the intersection.\n",
    "    \"\"\"\n",
    "\n",
    "    return [c for c in df.columns if c in target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Split the features to be processed by their types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#M.train_X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables that do not require preprocessing\n",
    "passthrough_cols = ['']\n",
    "\n",
    "# non-ordinal / categorical variables\n",
    "categorical_cols = [''] \n",
    "\n",
    "# quantitative variables\n",
    "quantitative_cols = set(M.train_X.select_dtypes(include=['float','int']).columns.tolist()) - set(passthrough_cols) - set(categorical_cols)\n",
    "quantitative_cols = list(quantitative_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Fast checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataframe contains NaN value\n",
    "test = M.train_X.isna().sum().to_frame('nb')\n",
    "test[test['nb'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Check if dataframe contains infinity\n",
    "test = np.isinf(M.train_X).sum().to_frame('nb')\n",
    "test[test['nb'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # prevalence\n",
    "y = M.train_y\n",
    "out = pd.concat([\n",
    "    np.round(y.value_counts(normalize=True, ascending=False).rename('normalized'), 4) * 100,\n",
    "    y.value_counts(normalize=False, ascending=False).rename('number'),\n",
    "], axis=1)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_preprocessor(passthrough_cols: list, categorical_cols: list, quantitative_cols: list, scale=False):\n",
    "    \"\"\"\n",
    "    Creates a data preprocessing pipeline\n",
    "\n",
    "    Args:\n",
    "        passthrough_cols (list):  list of variables that do not require preprocessing.\n",
    "        categorical_cols (list):  list of non-ordinal / categorical variables .\n",
    "        quantitative_cols (list): list of quantitative variables.\n",
    "        scale (boolean, optional): Should we standardize the variables? Defaults to False.\n",
    "    \"\"\"\n",
    "\n",
    "    # This step will make each preprocessing step robust if it does not receive all the assumed variables\n",
    "    passthrough_cols_selector = partial(_intersect, target=passthrough_cols)\n",
    "    categorical_cols_selector = partial(_intersect, target=categorical_cols)\n",
    "    quantitative_cols_selector = partial(_intersect, target=quantitative_cols)\n",
    "\n",
    "    # Make the preprocessor pipeline\n",
    "    lowvar_transformer = skpip.Pipeline(steps=[\n",
    "        ('NZV', NearZeroVar_filter(equisample=True))\n",
    "    ])\n",
    "\n",
    "    # Categorical-non-ordinal pipeline\n",
    "    categorical_transformer = skpip.Pipeline(steps=[\n",
    "        ('encoder', WOEEncoder())\n",
    "    ])\n",
    "\n",
    "    # Passthrough pipeline\n",
    "    passthrough_transformer = skpip.Pipeline(steps=[\n",
    "        ('passthrough', Passthrough())\n",
    "    ])\n",
    "\n",
    "    # Quantitative pipeline\n",
    "    quantitative_transformer = skpip.Pipeline(steps=[\n",
    "        ('low-var', LowVar_Filter(equisample=True)),\n",
    "        ('high-correlation', HighCorrelation_filter(equisample=True)),\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "    ])\n",
    "\n",
    "    if scale:\n",
    "        quantitative_transformer.steps.append(('scaler', StandardScaler()))\n",
    "\n",
    "\n",
    "    # Map the columns to their respective transformers\n",
    "    # ColumnTransformer allows to apply data transformations to different features of a same df. \n",
    "    # Columns not specified in the \"Transformers\" list are removed from the default dataset.\n",
    "    columns_transformers = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('quantitative', quantitative_transformer, quantitative_cols_selector),\n",
    "            ('passthrough', passthrough_transformer, passthrough_cols_selector),\n",
    "            ('categorical', categorical_transformer, categorical_cols_selector),\n",
    "        ],\n",
    "        verbose_feature_names_out=False,\n",
    "    )\n",
    "\n",
    "    # Define the pipeline\n",
    "    preprocessor = skpip.Pipeline(steps=[\n",
    "        ('lowvar_transformer', lowvar_transformer),\n",
    "        ('columns_transformers', columns_transformers),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SmokeTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = build_data_preprocessor(passthrough_cols=passthrough_cols, categorical_cols=categorical_cols, quantitative_cols=quantitative_cols, scale=False)\n",
    "preprocessor.fit(M.train_X, M.train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(preprocessor))\n",
    "print(type(preprocessor.named_steps['columns_transformers']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the columns names from the FINAL layer of the transformer\n",
    "preprocessor.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = preprocessor.transform(M.train_X)\n",
    "X_transformed = pd.DataFrame(X_transformed, columns=preprocessor.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_pipeline = skpip.Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LogisticAutoTuning(optuna_trials=10, optim_metric_strategy='aucpr'))      \n",
    "])\n",
    "\n",
    "logistic_pipeline.fit(M.train_X, M.train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_pipeline.named_steps['model'].model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta values in Logistic Regression\n",
    "logistic_pipeline.named_steps['model']._model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_pipeline.named_steps['model']._threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_pipeline  = skpip.Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', XGBoostAutoTuning(optuna_trials=10, optim_metric_strategy='aucpr'))      \n",
    "])\n",
    "\n",
    "xgboost_pipeline .fit(M.train_X, M.train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_pipeline .named_steps['model'].model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_pipeline .named_steps['model']._threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prevalence_rate(y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute the empirical prevalence rate\n",
    "    \"\"\"\n",
    "\n",
    "    y = pd.Series(y)\n",
    "    out = pd.concat([\n",
    "        np.round(y.value_counts(normalize=True, ascending=False).rename('normalized'), 4) * 100,\n",
    "        y.value_counts(normalize=False, ascending=False).rename('number'),\n",
    "    ], axis=1)\n",
    "    \n",
    "    return dp.Table(out, caption='Empirical prevalence rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_preprocessing_step(preprocessor: skpip.Pipeline, prevalence_group: dp.Table):\n",
    "    \"\"\"\n",
    "    Build the preprocessing group\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare the group of steps to be iterated over\n",
    "    groups = []\n",
    "\n",
    "    # Fetch the NZV group common to all variables\n",
    "    group = dp.Group(preprocessor\n",
    "            .named_steps['lowvar_transformer'] \\\n",
    "            .named_steps['NZV'] \\\n",
    "            .get_report_group()\n",
    "        )\n",
    "    groups.append(group)\n",
    "\n",
    "    # Iter over each group of preprocessor specific to the quantitative pipeline\n",
    "    for step in ('high-correlation', 'low-var'): \n",
    "        group = dp.Group(preprocessor\n",
    "            .named_steps['columns_transformers'] \\\n",
    "            .named_transformers_['quantitative'] \\\n",
    "            .named_steps[step] \\\n",
    "            .get_report_group()\n",
    "        )\n",
    "    \n",
    "        groups.append(group)\n",
    "    \n",
    "\n",
    "    # compute the final number of variables keeped by the model\n",
    "    cols = len(preprocessor.named_steps['columns_transformers'].get_feature_names_out())\n",
    "\n",
    "    return dp.Group(\n",
    "        dp.Text('# Data preprocessing report'),\n",
    "        dp.Text('## Estimated prevalence rate'),\n",
    "        prevalence_group,\n",
    "        dp.BigNumber(heading='Final number of variables used by the model after preprocessing', value=cols),\n",
    "        *groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SmokeTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence_group = compute_prevalence_rate(M.train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessing_group = report_preprocessing_step(preprocessor, prevalence_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as a static HTML-based report\n",
    "dp.save_report(preprocessing_group, path='../reports/html/prepro.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelisation-Q488yCbE-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
